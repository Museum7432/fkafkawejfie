{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create trainning data for xlmr-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 26967/26967 [00:00<00:00, 85439.75it/s]\n",
      "100%|███████████████████████████████████| 26967/26967 [00:14<00:00, 1903.81it/s]\n",
      "100%|███████████████████████████████████| 26967/26967 [00:14<00:00, 1923.64it/s]\n",
      "100%|████████████████████████████████████| 26967/26967 [03:02<00:00, 148.13it/s]\n",
      "100%|███████████████████████████████| 100847/100847 [00:00<00:00, 163080.29it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-train.json' \\\n",
    "    --claims_file datasets/xlmr_long/claims.jsonl \\\n",
    "    --corpus_file datasets/xlmr_long/corpus.jsonl \\\n",
    "    --tokenizer 'markussagen/xlm-roberta-longformer-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    # --sample_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 37967/37967 [00:00<00:00, 87043.27it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:19<00:00, 1937.55it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:34<00:00, 1114.04it/s]\n",
      "100%|████████████████████████████████████| 37967/37967 [04:37<00:00, 136.74it/s]\n",
      "100%|█████████████████████████████████| 115360/115360 [00:12<00:00, 9288.31it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-train.json' \\\n",
    "    --output_file datasets/xlmr_long/train.jsonl \\\n",
    "    --tokenizer 'markussagen/xlm-roberta-longformer-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --use_alternative_format \\\n",
    "    # --sample_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/dsc/datasets/xlmr_long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './custom*/*': No such file or directory\n",
      "173178802\n",
      "189402017\n",
      "/home/arch/Projects/college/dsc/datasets/xlmr_long/custom1\n",
      "3140398\n",
      "170038404\n",
      "/home/arch/Projects/college/dsc/datasets/xlmr_long/custom2\n",
      "3149166\n",
      "186252851\n",
      "/home/arch/Projects/college/dsc\n"
     ]
    }
   ],
   "source": [
    "%cd ./datasets/xlmr_long\n",
    "\n",
    "!mkdir ./custom1\n",
    "!mkdir ./custom2\n",
    "!rm ./custom*/*\n",
    "\n",
    "!csplit train.jsonl 55000\n",
    "!mv xx00 ./custom1/t\n",
    "!mv xx01 ./custom2/t\n",
    "\n",
    "%cd ./custom1\n",
    "\n",
    "!csplit t 1000\n",
    "!rm t\n",
    "\n",
    "!mv xx00 dev.jsonl\n",
    "!mv xx01 train.jsonl\n",
    "\n",
    "%cd ../custom2\n",
    "\n",
    "!csplit t 1000\n",
    "!rm t\n",
    "\n",
    "!mv xx00 dev.jsonl\n",
    "!mv xx01 train.jsonl\n",
    "\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create trainning data for phobert-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-28 12:02:52 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 93328.08it/s]\n",
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1790.79it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:18<00:00, 105.98it/s]\n",
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1925.73it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:14<00:00, 141.27it/s]\n",
      "100%|█████████████████████████████████████| 7450/7450 [00:00<00:00, 9577.76it/s]\n",
      "100%|███████████████████████████████████| 7450/7450 [00:00<00:00, 154243.46it/s]\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 810885.26it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-train.json' \\\n",
    "    --claims_file datasets/phober_long/claims.jsonl \\\n",
    "    --corpus_file datasets/phober_long/corpus.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --word_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-05 20:49:10 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████| 37967/37967 [00:00<00:00, 87750.07it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:12<00:00, 3161.04it/s]\n",
      "100%|████████████████████████████████████| 37967/37967 [06:07<00:00, 103.31it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:34<00:00, 1105.12it/s]\n",
      "100%|████████████████████████████████████| 37967/37967 [04:20<00:00, 145.56it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:04<00:00, 9135.62it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-train.json' \\\n",
    "    --output_file datasets/phober_long/train.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --word_segmented \\\n",
    "    --use_alternative_format \\\n",
    "    --max_number_of_slices 1 \\\n",
    "    # --sample_size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 21:18:19 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████| 36300/36300 [00:00<00:00, 84970.42it/s]\n",
      "100%|███████████████████████████████████| 36300/36300 [00:11<00:00, 3140.54it/s]\n",
      "100%|████████████████████████████████████| 36300/36300 [05:34<00:00, 108.53it/s]\n",
      "100%|███████████████████████████████████| 36300/36300 [00:32<00:00, 1122.78it/s]\n",
      "100%|████████████████████████████████████| 36300/36300 [04:08<00:00, 145.92it/s]\n",
      "100%|███████████████████████████████████| 75445/75445 [00:07<00:00, 9712.77it/s]\n",
      "done!\n",
      "2023-11-11 21:29:09 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|████████████████████████████████████| 1667/1667 [00:00<00:00, 78916.29it/s]\n",
      "100%|█████████████████████████████████████| 1667/1667 [00:01<00:00, 1623.93it/s]\n",
      "100%|██████████████████████████████████████| 1667/1667 [00:15<00:00, 108.16it/s]\n",
      "100%|█████████████████████████████████████| 1667/1667 [00:01<00:00, 1040.96it/s]\n",
      "100%|██████████████████████████████████████| 1667/1667 [00:11<00:00, 140.19it/s]\n",
      "100%|█████████████████████████████████████| 1667/1667 [00:00<00:00, 7743.14it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# for trainning\n",
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/dsc01-trainset.json' \\\n",
    "    --output_file datasets/phober_long/custom1/train.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --word_segmented \\\n",
    "    --use_alternative_format \\\n",
    "    --max_number_of_slices 2 \\\n",
    "    --mask_evidence \\\n",
    "    # --sample_size 5\n",
    "\n",
    "# for validating\n",
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/dsc01-valset.json' \\\n",
    "    --output_file datasets/phober_long/custom1/dev.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --word_segmented \\\n",
    "    --use_alternative_format \\\n",
    "    --max_number_of_slices 1 \\\n",
    "    # --sample_size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 21:36:58 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████| 36300/36300 [00:00<00:00, 84780.17it/s]\n",
      "100%|███████████████████████████████████| 36300/36300 [00:11<00:00, 3269.33it/s]\n",
      "100%|████████████████████████████████████| 36300/36300 [05:34<00:00, 108.56it/s]\n",
      "100%|███████████████████████████████████| 36300/36300 [00:32<00:00, 1105.22it/s]\n",
      "100%|████████████████████████████████████| 36300/36300 [04:11<00:00, 144.23it/s]\n",
      "100%|███████████████████████████████████| 36300/36300 [00:03<00:00, 9097.29it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# for trainning\n",
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/dsc01-trainset.json' \\\n",
    "    --output_file datasets/phober_long/train.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --for_trainning \\\n",
    "    --word_segmented \\\n",
    "    --use_alternative_format \\\n",
    "    --max_number_of_slices 1 \\\n",
    "    # --mask_evidence \\\n",
    "    # --sample_size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/dsc/datasets/phober_long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './custom*/*': No such file or directory\n",
      "6282408\n",
      "153259062\n",
      "/home/arch/Projects/college/dsc/datasets/phober_long/custom1\n",
      "/home/arch/Projects/college/dsc\n"
     ]
    }
   ],
   "source": [
    "%cd ./datasets/phober_long\n",
    "!mkdir ./custom1\n",
    "!mkdir ./custom2\n",
    "!rm ./custom*/*\n",
    "\n",
    "!csplit train.jsonl 1500\n",
    "\n",
    "!mv xx0* ./custom1/\n",
    "\n",
    "%cd ./custom1\n",
    "\n",
    "!mv xx00 dev.jsonl\n",
    "\n",
    "!mv xx01 train.jsonl\n",
    "\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/dsc/datasets/phober_long\n",
      "mkdir: cannot create directory ‘./custom1’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./custom2’: File exists\n",
      "152188968\n",
      "154172313\n",
      "/home/arch/Projects/college/dsc/datasets/phober_long/custom1\n",
      "4927923\n",
      "147261045\n",
      "/home/arch/Projects/college/dsc/datasets/phober_long/custom2\n",
      "4975434\n",
      "149196879\n",
      "/home/arch/Projects/college/dsc\n"
     ]
    }
   ],
   "source": [
    "%cd ./datasets/phober_long\n",
    "\n",
    "!mkdir ./custom1\n",
    "!mkdir ./custom2\n",
    "!rm ./custom*/*\n",
    "\n",
    "!csplit train.jsonl 46000\n",
    "!mv xx00 ./custom1/t\n",
    "!mv xx01 ./custom2/t\n",
    "\n",
    "%cd ./custom1\n",
    "\n",
    "!csplit t 1500\n",
    "!rm t\n",
    "\n",
    "!mv xx00 dev.jsonl\n",
    "!mv xx01 train.jsonl\n",
    "\n",
    "%cd ../custom2\n",
    "\n",
    "!csplit t 1500\n",
    "!rm t\n",
    "\n",
    "!mv xx00 dev.jsonl\n",
    "!mv xx01 train.jsonl\n",
    "\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-06 10:40:29 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|████████████████████████████████████| 5396/5396 [00:00<00:00, 83168.15it/s]\n",
      "100%|█████████████████████████████████████| 5396/5396 [00:02<00:00, 2033.16it/s]\n",
      "100%|██████████████████████████████████████| 5396/5396 [00:48<00:00, 110.37it/s]\n",
      "100%|██████████████████████████████████████| 5396/5396 [00:36<00:00, 148.09it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-private-test-offcial.json' \\\n",
    "    --claims_file datasets/test/claims_pt.jsonl \\\n",
    "    --corpus_file datasets/test/corpus_pt.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --word_segmented \\\n",
    "    --max_number_of_slices 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-04 11:07:26 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████| 37967/37967 [00:00<00:00, 87128.95it/s]\n",
      "100%|███████████████████████████████████| 37967/37967 [00:12<00:00, 3126.81it/s]\n",
      "100%|████████████████████████████████████| 37967/37967 [05:57<00:00, 106.07it/s]\n",
      "100%|████████████████████████████████████| 37967/37967 [04:18<00:00, 146.82it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-train.json' \\\n",
    "    --claims_file datasets/test/claims_train.jsonl \\\n",
    "    --corpus_file datasets/test/corpus_train.jsonl \\\n",
    "    --tokenizer 'bluenguyen/longformer-phobert-base-4096' \\\n",
    "    --word_segmented \\\n",
    "    --max_number_of_slices 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4794/4794 [00:00<00:00, 99047.30it/s]\n",
      "100%|█████████████████████████████████████| 4794/4794 [00:02<00:00, 1654.28it/s]\n",
      "100%|██████████████████████████████████████| 4794/4794 [00:35<00:00, 133.37it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "!python scripts/to_multivers_format.py \\\n",
    "    --input_file './datasets/ise-dsc01-public-test-offcial.json' \\\n",
    "    --claims_file datasets/test/claims_XLMR.jsonl \\\n",
    "    --corpus_file datasets/test/corpus_XLMR.jsonl \\\n",
    "    --tokenizer 'markussagen/xlm-roberta-longformer-base-4096' \\\n",
    "    --max_number_of_slices 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./datasets/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/dsc/dev/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|███████████████████████████████████| 2397/2397 [00:00<00:00, 285488.04it/s]\n",
      "/home/arch/Projects/college/dsc/dev/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|█████████████████████████████████████| 239/239 [00:00<00:00, 290646.17it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/translate.py --claims_file datasets/claims.jsonl --corpus_file datasets/corpus.jsonl --output_dir datasets/outputs --chunk_size_claim 20 --chunk_size_corpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaLongModel were not initialized from the model checkpoint at bluenguyen/longformer-phobert-base-4096 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 91/91 [00:58<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python src/predict.py \\\n",
    "        --checkpoint_path=checkpoints/2722.ckpt \\\n",
    "        --input_file=datasets/test/claims_pt2.jsonl \\\n",
    "        --corpus_file=datasets/test/corpus_pt.jsonl \\\n",
    "        --output_file=outputs/test1.jsonl \\\n",
    "        --batch_size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python multivers/predict.py \\\n",
    "        --checkpoint_path=\"../../../test.ckpt\" \\\n",
    "        --input_file=\"../../../test/claims_pt.jsonl\" \\\n",
    "        --corpus_file=\"../../../test/corpus_pt.jsonl\" \\\n",
    "        --output_file=\"../../../outputs/output_lphobertpt.jsonl\" \\\n",
    "        --batch_size 4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/predict.py \\\n",
    "        --checkpoint_path=multivers/checkpoints/fever_sci.ckpt \\\n",
    "        --input_file=datasets/outputs/claims.jsonl \\\n",
    "        --corpus_file=datasets/outputs/corpus400.jsonl \\\n",
    "        --output_file=outputs/output_fever_sci400.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/predict.py \\\n",
    "        --checkpoint_path=multivers/checkpoints/fever.ckpt \\\n",
    "        --input_file=datasets/outputs/claims.jsonl \\\n",
    "        --corpus_file=datasets/outputs/corpus.jsonl \\\n",
    "        --output_file=outputs/output_fever.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arch/Projects/college/dsc/multivers\n",
      "Global seed set to 76\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/arch/Projects/college/dsc/dev_multivers/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Experiment logs directory ./checkpoints_user/fever_2 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name                 | Type            | Params\n",
      "---------------------------------------------------------\n",
      "0 | encoder              | XLMRobertaModel | 559 M \n",
      "1 | dropout              | Dropout         | 0     \n",
      "2 | label_classifier     | FeedForward     | 1.1 M \n",
      "3 | rationale_classifier | FeedForward     | 2.1 M \n",
      "4 | metrics              | ModuleDict      | 0     \n",
      "---------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "559 M     Non-trainable params\n",
      "563 M     Total params\n",
      "2,252.169 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|    | 28/345916 [00:06<23:58:23,  4.01it/s, loss=28.2, v_num=er_2]^C\n",
      "/home/arch/Projects/college/dsc/dev_multivers/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint...\n",
      "Epoch 0:   0%|    | 28/345916 [00:07<25:12:13,  3.81it/s, loss=28.2, v_num=er_2]\n"
     ]
    }
   ],
   "source": [
    "%cd multivers\n",
    "!python script/pretrain.py --datasets fever --gpus \"1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
